{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985c890c",
   "metadata": {},
   "source": [
    "### acc 85% 이상으로 구현해보자\n",
    "\n",
    "# Convolutional AutoEncoder (CAE) + Classifier \n",
    "\n",
    "- Hybrid Model : CAE 를 통한 image reconstructor와 CAE 내부의 encoder를 통해 classification을 동시에 진행 -> 성능 향상\n",
    "\n",
    "        다중 작업 학습(Multi-Task Learning) 이라고 칭하기도 함\n",
    "\n",
    "- 구조 : CAE 의 encoder를 공유하여 학습 데이터(image) 를 latent vector로 압축하고, latent vector를 decoder와 classifer가 공유함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4976d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, RandomCrop, ColorJitter, RandomRotation\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Module, Sequential, Linear, Conv2d, ReLU, MaxPool2d, ConvTranspose2d, Tanh, MSELoss, CrossEntropyLoss, Flatten, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "from time import time\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673625a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2) : 색상 무작위 변경 (밝기, 대비, 채도)\n",
    "# transforms.RandomRotation(15) : 무작위 회전 (-15 ~ 15도)\n",
    "transform_train = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomCrop(32, padding=4),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    RandomRotation(15),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train = CIFAR10(root='data_cifar10', train=True, download=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test = CIFAR10(root='data_cifar10', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6ff565",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "trainset = DataLoader(train, batch_size=128, shuffle=True)\n",
    "testset = DataLoader(test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoEncoder(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        latent_space = 256\n",
    "\n",
    "        self.encoder = Sequential(\n",
    "            # 32 * 32 * 3 -> 32 * 32 * 32 (channel 증가 : 3-> 32)\n",
    "            Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            BatchNorm2d(32),\n",
    "            ReLU(),\n",
    "            # 16 * 16 * 32 (image downsampling)\n",
    "            MaxPool2d(2, 2),\n",
    "\n",
    "            # 16 * 16 * 64 (channel 증가 32 -> 64)\n",
    "            Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            BatchNorm2d(64),\n",
    "            ReLU(),\n",
    "            # 8 * 8 * 64 (image downsampling)\n",
    "            MaxPool2d(2, 2),\n",
    "\n",
    "            # 8 * 8 * 128 (channel 증가 : 64 -> 128)\n",
    "            Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            BatchNorm2d(128),\n",
    "            ReLU(),\n",
    "            # 4 * 4 * 128 (image downsampling)\n",
    "            MaxPool2d(2, 2),\n",
    "            \n",
    "            # 4 * 4 * 256 (channel 증가 : 128 -> 256)\n",
    "            Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            BatchNorm2d(256),\n",
    "            ReLU(),\n",
    "            # 2 * 2 * 256 (image downsampling)\n",
    "            MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.encoder_linear = Sequential(\n",
    "            # latent layer로 입력하기 위해 flatten\n",
    "            Flatten(),\n",
    "            Linear(2 * 2 * 256, latent_space),\n",
    "            ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder_linear = Sequential(\n",
    "            # latent layer에 입력되던 image shape으로 변경\n",
    "            Linear(latent_space, 2 * 2 * 256),\n",
    "            ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = Sequential(\n",
    "            # ConvTranspose2d : 전치 합성곱 (pooling으로 줄어든 image를 upsampling -> convolution을 거꾸로)\n",
    "            # 2 * 2 * 256 -> 4 * 4 * 128\n",
    "            ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            BatchNorm2d(128),\n",
    "            ReLU(),\n",
    "            # 4 * 4 * 128 -> 8 * 8 * 64\n",
    "            ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            BatchNorm2d(64),\n",
    "            ReLU(),\n",
    "            # 8 * 8 * 64 -> 16 * 16  * 32\n",
    "            ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            BatchNorm2d(32),\n",
    "            ReLU(),\n",
    "            # 16 * 16  * 32 -> 32 * 32 * 3\n",
    "            ConvTranspose2d(32, 3, kernel_size=2, stride=2),\n",
    "            # 마지막 convTranspose에서는 batch normalize 사용 x\n",
    "            Tanh()        \n",
    "        )\n",
    "\n",
    "        self.classifier = Sequential(\n",
    "            Linear(latent_space, 512),\n",
    "            ReLU(),\n",
    "            Dropout(0.3),\n",
    "            Linear(512, 256),\n",
    "            ReLU(),\n",
    "            Dropout(0.3),\n",
    "            Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = self.encoder(x)\n",
    "        latent_vector = self.encoder_linear(feature_map)\n",
    "        # classification    \n",
    "        logits = self.classifier(latent_vector)\n",
    "        \n",
    "        decoder_input = self.decoder_linear(latent_vector)\n",
    "        # reconstruction\n",
    "        # .view(batch_size, channel, height, weight) -> -1 을 통해 batch size는 자동으로\n",
    "        reconstructed_img = self.decoder(decoder_input.view(-1, 256, 2, 2))\n",
    "        \n",
    "        # 오토인코더 학습(image reconstruct)과 분류 학습(classification)을 위해 두 결과를 모두 반환\n",
    "        return reconstructed_img, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6fd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf24dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder 에서 나온 출력을 가지고 분류\n",
    "classification_loss_function = CrossEntropyLoss()\n",
    "# decoder 에서 나온 출력을 가지고 학습\n",
    "reconstruction_loss_function = MSELoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "# weight_decay : L2 정규화 \n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=0.000001)\n",
    "# step_size=10 : 10 epoch 마다\n",
    "# gamma=0.5 : learning_rate를 50% 줄이자 (0.001 -> 0.0005 -> 0.00025 ...) -> fine-tuning (학습을 세밀하게 조정)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3ba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \t Loss: 1.452 \t 56s\n",
      "epoch: 2 \t Loss: 1.119 \t 58s\n",
      "epoch: 3 \t Loss: 0.993 \t 59s\n",
      "epoch: 4 \t Loss: 0.895 \t 60s\n",
      "epoch: 5 \t Loss: 0.833 \t 64s\n",
      "epoch: 6 \t Loss: 0.783 \t 62s\n",
      "epoch: 7 \t Loss: 0.750 \t 63s\n",
      "epoch: 8 \t Loss: 0.723 \t 60s\n",
      "epoch: 9 \t Loss: 0.693 \t 62s\n",
      "epoch: 10 \t Loss: 0.671 \t 60s\n",
      "epoch: 11 \t Loss: 0.601 \t 60s\n",
      "epoch: 12 \t Loss: 0.576 \t 60s\n",
      "epoch: 13 \t Loss: 0.565 \t 60s\n",
      "epoch: 14 \t Loss: 0.553 \t 62s\n",
      "epoch: 15 \t Loss: 0.545 \t 68s\n",
      "epoch: 16 \t Loss: 0.528 \t 73s\n",
      "epoch: 17 \t Loss: 0.528 \t 62s\n",
      "epoch: 18 \t Loss: 0.515 \t 68s\n",
      "epoch: 19 \t Loss: 0.507 \t 73s\n",
      "epoch: 20 \t Loss: 0.502 \t 74s\n",
      "epoch: 21 \t Loss: 0.465 \t 63s\n",
      "epoch: 22 \t Loss: 0.455 \t 58s\n",
      "epoch: 23 \t Loss: 0.451 \t 59s\n",
      "epoch: 24 \t Loss: 0.445 \t 58s\n",
      "epoch: 25 \t Loss: 0.437 \t 59s\n",
      "epoch: 26 \t Loss: 0.440 \t 59s\n",
      "epoch: 27 \t Loss: 0.430 \t 72s\n",
      "epoch: 28 \t Loss: 0.429 \t 71s\n",
      "epoch: 29 \t Loss: 0.424 \t 75s\n",
      "epoch: 30 \t Loss: 0.421 \t 64s\n",
      "epoch: 31 \t Loss: 0.400 \t 62s\n",
      "epoch: 32 \t Loss: 0.397 \t 67s\n",
      "epoch: 33 \t Loss: 0.391 \t 68s\n",
      "epoch: 34 \t Loss: 0.390 \t 66s\n",
      "epoch: 35 \t Loss: 0.388 \t 72s\n",
      "epoch: 36 \t Loss: 0.384 \t 67s\n",
      "epoch: 37 \t Loss: 0.383 \t 69s\n",
      "epoch: 38 \t Loss: 0.379 \t 71s\n",
      "epoch: 39 \t Loss: 0.380 \t 66s\n",
      "epoch: 40 \t Loss: 0.376 \t 72s\n",
      "epoch: 41 \t Loss: 0.366 \t 66s\n",
      "epoch: 42 \t Loss: 0.364 \t 71s\n",
      "epoch: 43 \t Loss: 0.365 \t 71s\n",
      "epoch: 44 \t Loss: 0.365 \t 64s\n",
      "epoch: 45 \t Loss: 0.361 \t 72s\n",
      "epoch: 46 \t Loss: 0.366 \t 67s\n",
      "epoch: 47 \t Loss: 0.359 \t 74s\n",
      "epoch: 48 \t Loss: 0.355 \t 75s\n",
      "epoch: 49 \t Loss: 0.356 \t 76s\n",
      "epoch: 50 \t Loss: 0.357 \t 77s\n",
      "learning time (50 epoch) : 54m 55\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "# 각각의 loss에 가중치를 곱하기 위함 (더해서 1이 되도록)\n",
    "# 분류에 가중치를 더 많이 주자\n",
    "classification_weights = 0.9\n",
    "reconstruction_weights = 0.1\n",
    "\n",
    "total_time = list()\n",
    "for epoch in range(epochs):\n",
    "    now = time()\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainset):\n",
    "        x_train, y_train = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        reconstructed_img, logits = model(x_train)\n",
    "        # 각각의 loss 계산\n",
    "        class_loss = classification_loss_function(logits, y_train)\n",
    "        recon_loss = reconstruction_loss_function(reconstructed_img, x_train)\n",
    "        # 계산된 loss를 가중치로 곱하여, 어떤 작업을 중점으로 하여 학습할지를 결정\n",
    "        total_loss = (classification_weights * class_loss) + (reconstruction_weights * recon_loss)\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += total_loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    learning_time = int(time() - now)\n",
    "    total_time.append(learning_time)\n",
    "\n",
    "    print(f\"epoch: {epoch+1:3d}/{epochs} \\t Loss: {avg_loss/len(trainset):.3f} \\t {learning_time}s\")\n",
    "\n",
    "print(f\"learning time ({epochs} epoch) : {floor(sum(total_time) / 60)}m {sum(total_time) % 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "951208f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(testset)\n",
    "x_test, y_test = next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec06f4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'frog', 'dog', 'plane', 'horse', 'car', 'ship', 'car', 'truck', 'deer', 'bird', 'dog', 'plane', 'cat', 'car', 'plane', 'plane', 'truck', 'ship', 'dog', 'frog', 'deer', 'car', 'deer', 'truck', 'horse', 'dog', 'bird', 'frog', 'dog', 'horse', 'ship', 'ship', 'frog', 'truck', 'bird', 'truck', 'ship', 'truck', 'plane', 'deer', 'truck', 'dog', 'deer', 'deer', 'plane', 'cat', 'truck', 'car', 'deer', 'plane', 'horse', 'ship', 'deer', 'cat', 'truck', 'bird', 'deer', 'plane', 'car', 'plane', 'ship', 'plane', 'bird', 'truck', 'car', 'ship', 'dog', 'dog', 'cat', 'dog', 'ship', 'plane', 'horse', 'dog', 'bird', 'truck', 'horse', 'ship', 'cat', 'truck', 'bird', 'frog', 'cat', 'frog', 'frog', 'horse', 'cat', 'frog', 'horse', 'cat', 'plane', 'dog', 'horse', 'plane', 'dog', 'cat', 'horse', 'frog', 'cat', 'plane', 'car', 'ship', 'ship', 'cat', 'ship', 'ship', 'bird', 'deer', 'horse', 'horse', 'bird', 'plane', 'horse', 'deer', 'frog', 'plane', 'horse', 'plane', 'deer', 'truck', 'ship', 'truck', 'horse', 'car', 'dog', 'car', 'dog']\n",
      "['bird', 'frog', 'cat', 'plane', 'horse', 'car', 'ship', 'car', 'truck', 'frog', 'plane', 'dog', 'plane', 'cat', 'truck', 'plane', 'bird', 'truck', 'ship', 'frog', 'frog', 'deer', 'car', 'deer', 'truck', 'bird', 'dog', 'bird', 'dog', 'dog', 'horse', 'ship', 'ship', 'frog', 'truck', 'deer', 'truck', 'ship', 'truck', 'plane', 'deer', 'ship', 'cat', 'deer', 'bird', 'plane', 'cat', 'cat', 'car', 'bird', 'plane', 'horse', 'ship', 'deer', 'plane', 'car', 'bird', 'deer', 'ship', 'car', 'plane', 'ship', 'plane', 'bird', 'truck', 'car', 'plane', 'dog', 'dog', 'cat', 'dog', 'ship', 'plane', 'horse', 'dog', 'deer', 'truck', 'horse', 'ship', 'deer', 'truck', 'bird', 'frog', 'ship', 'frog', 'frog', 'horse', 'cat', 'frog', 'horse', 'cat', 'plane', 'dog', 'horse', 'plane', 'dog', 'cat', 'horse', 'frog', 'cat', 'plane', 'car', 'ship', 'ship', 'cat', 'ship', 'ship', 'bird', 'deer', 'horse', 'horse', 'bird', 'plane', 'horse', 'deer', 'frog', 'plane', 'horse', 'plane', 'deer', 'truck', 'ship', 'truck', 'horse', 'car', 'dog', 'car', 'dog']\n"
     ]
    }
   ],
   "source": [
    "name_list = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "_, predict = model(x_test)\n",
    "_, predict_labels = torch.max(predict, 1)\n",
    "\n",
    "print(list(map(lambda x: name_list[x], predict_labels)))\n",
    "print(list(map(lambda x: name_list[x], y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173b409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc : 85.27%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        x_test, y_test = data\n",
    "        _, h = model(x_test)\n",
    "        _, predicted = torch.max(h.data, 1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "\n",
    "print(f'acc : {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb8317",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
