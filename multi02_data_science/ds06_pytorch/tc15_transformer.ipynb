{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ff3495",
   "metadata": {},
   "source": [
    "# Transformer - Attention Is All You Need\n",
    "\n",
    "구조 : Encoder (self-attention + feed forward network) -> Decoder(self-attention + encoder-decoder attention + feed forward network) -> Fully Connected Layer\n",
    "\n",
    "- Encoder : 입력 문맥 파악 및 정보를 압축\n",
    "\n",
    "    - self-attention : input sequence 의 각 단어들을 가지고 Q (qeury), K (key), V (value) 계산 (각각의 Vector) = 단어들의 관련성 파악 \n",
    "        \n",
    "        -> 표현 계산 (q,k의 내적 계산 = 유사도 -> softmax -> 결과를 가중치로 변환 -> 가중치 * v) = 문맥에 맞는 단어 학습 (새로운 표현)\n",
    "        \n",
    "        -> 이 때 multi-head attention (여러 개의 attention head가 각각 q,k,v를 계산하고 결과를 결합) = attention head가 여러개 이기 때문에 다양한 표현 학습\n",
    "\n",
    "    - feed forward network (= 2개의 fully connected layer) : self-attention의 결과 표현을 비 선형적 변환 (dim 확장 -> dim 축소) = 복잡한 관계 학습\n",
    "\n",
    "\n",
    "- Decoder : 인코더 정보와 이전 출력 기반으로 출력 생성\n",
    "\n",
    "    - self-attention (masked self-attention) : output sequence 의 timestep-1 만 고려 -> 현재 단어 이후의 정보를 참조하지 못하도록 = 문맥 파악\n",
    "\n",
    "        -> 현재 입력을 가지고 q, k, v 계산\n",
    "\n",
    "    - encoder-decoder attention (cross-attention) : encoder의 출력(문맥)을 가지고 output sequence 생성 -> Encoder의 압축된 문맥을 참고하여 단어 생성\n",
    "\n",
    "        -> q (masked self-attention의 출력), k (encoder의 출력), v (encoder의 출력) 을 가지고 계산\n",
    "    \n",
    "    - feed forward network : encoder와 동일\n",
    "\n",
    "\n",
    "- Fully Connected Layer : 차원을 변환하여 단어 예측\n",
    "\n",
    "\n",
    "\n",
    "- 각 부분 연결 시 Residual Connection (잔차 연결), Layer Normalization (레이어 정규화), Positional Encoding (위치 정보) 필요\n",
    "\n",
    "    * RC & LN : Residual Connection & Layer Normalization\n",
    "\n",
    "    Encoder (embedding + positional encoding -> self-attention -> rc & ln -> feed forward network -> rc & ln)\n",
    "\n",
    "    ->\n",
    "    \n",
    "    Decoder(embedding + positional encoding -> masked self-attention -> rc & ln -> Encoder-Decoder Attention -> rc & ln -> feed forward network -> rc & ln)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143ef413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Embedding, Dropout, Linear, CrossEntropyLoss, init, utils, LayerNorm, ModuleList\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from time import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d8a179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else: \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e5c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"wmt16\", \"de-en\", split=\"train[:10%]\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7816c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized_en = tokenizer(\n",
    "        [ex['en'] for ex in examples['translation']],\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    tokenized_de = tokenizer(\n",
    "        [ex['de'] for ex in examples['translation']],\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tokenized_en['input_ids'],\n",
    "        'attention_mask': tokenized_en['attention_mask'],\n",
    "        'labels': tokenized_de['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6aeb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "train_data = train_test_split['train']\n",
    "valid_data = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4220bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_iterator = DataLoader(valid_data, batch_size=batch_size)\n",
    "\n",
    "input_dim = tokenizer.vocab_size\n",
    "output_dim = tokenizer.vocab_size\n",
    "pad_idx = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865b5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token들의 위치 정보를 model에 주입\n",
    "class PositionalEncoding(Module):\n",
    "    # model_dim : model의 embedding dim\n",
    "    # max_len : encoding 할 수 있는 sequence 최대 길이\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # 위치 인코딩 행렬\n",
    "        position_encoding_matrics = torch.zeros(max_len, model_dim)\n",
    "        \n",
    "        # position tensor\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # sin / cos 계산의 분모\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
    "        # pe (positional encoding matrics) 에 \n",
    "        position_encoding_matrics[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding_matrics[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # batch  dimension 추가 -> 3차원 tensor\n",
    "        position_encoding_matrics = position_encoding_matrics.unsqueeze(0)\n",
    "        # parameter가 아니라 상태(state)값으로 저장 -> 학습 X \n",
    "        self.register_buffer('position_encoding_matrics', position_encoding_matrics)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input sequence의 길이로 잘라서 리턴\n",
    "        return self.position_encoding_matrics[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2fee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 생성 (sequence -> token -> attention)\n",
    "class MultiHeadAttention(Module):\n",
    "    def __init__(self, model_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # input / output embedding 차원\n",
    "        self.model_dim = model_dim\n",
    "        # attention을 생성할 head의 갯수\n",
    "        self.n_heads = n_heads\n",
    "        # head의 차원\n",
    "        self.head_dim = model_dim // n_heads \n",
    "        \n",
    "        # layer 생성\n",
    "        # query layer\n",
    "        self.fc_q = Linear(model_dim, model_dim)\n",
    "        # key layer\n",
    "        self.fc_k = Linear(model_dim, model_dim)\n",
    "        # value layer\n",
    "        self.fc_v = Linear(model_dim, model_dim)\n",
    "        # output layer\n",
    "        self.fc_o = Linear(model_dim, model_dim)\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        # attention score를 계산하기 위한 scale vector\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # vector 생성\n",
    "        q = self.fc_q(query)\n",
    "        k = self.fc_k(key)\n",
    "        v = self.fc_v(value)\n",
    "        \n",
    "        # 4차원으로 변환 -> head별 병렬 계산을 위해\n",
    "        q = q.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # q, k scaling\n",
    "        energy = torch.matmul(q, k.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            # -1e9 (매우 작은 음수) 를 채워서 softmax 가 계산 시 attention weights 를 0으로 변경\n",
    "            # 0으로 채우면 softmax 가 계산할 때 1 (e^0) 이 되버림 \n",
    "            energy = energy.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # attention weights -> softmax 계산\n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        \n",
    "        # attention weights * v = 문맥에 맞는 단어 학습\n",
    "        x = torch.matmul(self.dropout(attention), v)\n",
    "        \n",
    "        # contiguous : memory 재정렬\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # 모든 head의 계산 결과를 결합 (concatenate)\n",
    "        x = x.reshape(batch_size, -1, self.model_dim)\n",
    "        \n",
    "        # output layer를 통해 정렬\n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        # 최종 출력과 attention weights 리턴\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fa132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positionwise : 모든 postion에 대해 동일한 가중치 계산\n",
    "# attention 은 다른 position의 vector (token) 의 관계를 함께 계산하지만, \n",
    "# feedforward 에서는 각각의 vector (token)들이 독립적으로 가중치 계산 -> 표현 (특징 = feature) 을 뚜렷하게 (sharpen) 한다. \n",
    "class PositionwiseFeedforward(Module):\n",
    "    def __init__(self, model_dim, feedforward_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 확장\n",
    "        self.fc_1 = Linear(model_dim, feedforward_dim)\n",
    "        # 축소\n",
    "        self.fc_2 = Linear(feedforward_dim, model_dim)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ef0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Connection & Layer Normalization -> 논문에서의 구조 (잔차 연결 -> 정규화)\n",
    "class SublayerConnection(Module):\n",
    "    def __init__(self, model_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(model_dim)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759733d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization & Residual Connection -> 정규화를 먼저 한 이후에 잔차 연결 (학습이 더 안정적으로 동작)\n",
    "class SublayerConnection_PreNorm(Module):\n",
    "    def __init__(self, model_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(model_dim)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae5f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder에서 반복되는 부분을 encapsulation\n",
    "class EncoderLayer(Module):\n",
    "    def __init__(self, model_dim, n_heads, feedforward_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self-attention layer\n",
    "        self.self_attention = MultiHeadAttention(model_dim, n_heads, dropout)\n",
    "        # LN & RC\n",
    "        self.self_attention_sublayer = SublayerConnection_PreNorm(model_dim, dropout) \n",
    "        # feed forward layer\n",
    "        self.feed_forward = PositionwiseFeedforward(model_dim, feedforward_dim, dropout)\n",
    "        # LN & RC \n",
    "        # 둘 다 ln * rc 인데 두 개 만드는 이유 : 독립적으로 동작시키기 위해 (독립적인 가중치)\n",
    "        self.feedforward_sublayer = SublayerConnection_PreNorm(model_dim, dropout)\n",
    "        \n",
    "    # src : soruce sequence\n",
    "    # src_mask : src의 padding token을 masking\n",
    "    def forward(self, src, src_mask):\n",
    "        # ln(정규화) -> attention 계산 -> rc(잔차 연결)\n",
    "        src = self.self_attention_sublayer(src, lambda x: self.self_attention(x, x, x, src_mask)[0])\n",
    "        # ln -> ff -> rc\n",
    "        src = self.feedforward_sublayer(src, self.feed_forward)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7d78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "    def __init__(self, input_dim, model_dim, n_layers, n_heads, feedforward_dim, dropout, max_len=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input sentence -> token\n",
    "        self.tok_embedding = Embedding(input_dim, model_dim, padding_idx=pad_idx)\n",
    "        # token + position 정보\n",
    "        self.position_embedding = PositionalEncoding(model_dim, max_len)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        # n개의 EncoderLayer (반복되는 코드 블록)\n",
    "        self.layers = ModuleList([\n",
    "            EncoderLayer(model_dim, n_heads, feedforward_dim, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # ln (마지막 출력) -> decoder의 k,v 로 사용\n",
    "        self.norm = LayerNorm(model_dim)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        embedded = self.tok_embedding(src)\n",
    "        position_encoding = self.position_embedding(embedded)\n",
    "        src = self.dropout(embedded + position_encoding)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        return self.norm(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c91858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(Module):\n",
    "    def __init__(self, model_dim, n_heads, feedforward_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # masked self-attention\n",
    "        self.self_attention = MultiHeadAttention(model_dim, n_heads, dropout)\n",
    "        self.self_attention_sublayer = SublayerConnection_PreNorm(model_dim, dropout)\n",
    "        \n",
    "        # encoder-decoder attention (cross attention)\n",
    "        self.cross_attention = MultiHeadAttention(model_dim, n_heads, dropout)\n",
    "        self.cross_attention_sublayer = SublayerConnection_PreNorm(model_dim, dropout)\n",
    "        \n",
    "        # encoder와 동일\n",
    "        self.feed_forward = PositionwiseFeedforward(model_dim, feedforward_dim, dropout)\n",
    "        self.feedforward_sublayer = SublayerConnection_PreNorm(model_dim, dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        trg = self.self_attention_sublayer(trg, lambda x: self.self_attention(x, x, x, trg_mask)[0])\n",
    "        trg = self.cross_attention_sublayer(trg, lambda x: self.cross_attention(x, enc_src, enc_src, src_mask)[0])\n",
    "        trg = self.feedforward_sublayer(trg, self.feed_forward)\n",
    "        \n",
    "        return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c59406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Module):\n",
    "    def __init__(self, output_dim, model_dim, n_layers, n_heads, feedforward_dim, dropout, max_len=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_embedding = Embedding(output_dim, model_dim, padding_idx=pad_idx)\n",
    "        self.pos_embedding = PositionalEncoding(model_dim, max_len)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        self.layers = ModuleList([\n",
    "            DecoderLayer(model_dim, n_heads, feedforward_dim, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder only 모델에서는 Decoder 내부에 FC layer 존재\n",
    "        # self.fully_connected_layer = Linear(model_dim, output_dim)\n",
    "        self.norm = LayerNorm(model_dim)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        embedded = self.tok_embedding(trg)\n",
    "        pos_enc = self.pos_embedding(embedded)\n",
    "        trg = self.dropout(embedded + pos_enc)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        trg = self.norm(trg)\n",
    "        # output = self.fully_connected_layer(trg)\n",
    "        \n",
    "        return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2102e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src, trg, pad_idx):\n",
    "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    trg_pad_mask = (trg != pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "    \n",
    "    trg_len = trg.shape[1]\n",
    "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
    "    \n",
    "    trg_mask = trg_pad_mask & trg_sub_mask\n",
    "    \n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f42f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Module):\n",
    "    def __init__(self, model_dim, output_dim, encoder, decoder, src_pad_idx, trg_pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.fully_connected_layer = Linear(model_dim, output_dim)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        src_mask, trg_mask = create_masks(src, trg, self.src_pad_idx)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        output = self.fully_connected_layer(dec_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49fa6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 512\n",
    "n_heads = 8\n",
    "n_layers = 3\n",
    "feedforward_dim = model_dim * 4\n",
    "dropout = 0.1\n",
    "max_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16066457",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim, model_dim, n_layers, n_heads, feedforward_dim, dropout, max_len)\n",
    "decoder = Decoder(output_dim, model_dim, n_layers, n_heads, feedforward_dim, dropout, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b39908d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(model_dim, output_dim, encoder, decoder, pad_idx, pad_idx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6395a90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(58101, 512, padding_idx=58100)\n",
       "    (position_embedding): PositionalEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attention_sublayer): SublayerConnection_PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedforward(\n",
       "          (fc_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward_sublayer): SublayerConnection_PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(58101, 512, padding_idx=58100)\n",
       "    (pos_embedding): PositionalEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attention_sublayer): SublayerConnection_PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_attention): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_attention_sublayer): SublayerConnection_PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedforward(\n",
       "          (fc_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward_sublayer): SublayerConnection_PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fully_connected_layer): Linear(in_features=512, out_features=58101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 초기화로 xavier uniform 사용\n",
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dim() > 1:\n",
    "            init.xavier_uniform_(param)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edfb4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "# warmup 단계동안 증가, 그 후 역제곱근(inverse square root) 에 따라 감소\n",
    "class NoamOptimizer:\n",
    "    def __init__(self, model_dim, warmup_steps, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.model_dim = model_dim\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self._step = 0\n",
    "        self._rate = 0.\n",
    "\n",
    "    # 각 batch마다 호출\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "\n",
    "        for param in self.optimizer.param_groups:\n",
    "            param['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # 현재 step에 대한 lr 계산\n",
    "    def rate(self, step=None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        \n",
    "        scale = self.model_dim ** (-0.5)\n",
    "        \n",
    "        return scale * min(step ** (-0.5), step * (self.warmup_steps ** (-1.5)))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a21866d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "epsilon = 1e-9\n",
    "betas=(0.9, 0.98)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, betas=betas, eps=epsilon)\n",
    "\n",
    "warmup_steps = 4000\n",
    "optimizer = NoamOptimizer(model_dim, warmup_steps, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29e6d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_function):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = len(iterator)\n",
    "    total_batch_time = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        batch_start_time = time()\n",
    "\n",
    "        src = batch['input_ids'].to(device)\n",
    "        trg = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:, :-1])\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        \n",
    "        trg = trg[:, 1:].reshape(-1) \n",
    "        \n",
    "        loss = loss_function(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        batch_end_time = time()\n",
    "        batch_time = batch_end_time - batch_start_time\n",
    "        total_batch_time += batch_time\n",
    "\n",
    "        batch_mins = int(total_batch_time / 60)\n",
    "        batch_secs = int(total_batch_time% 60)\n",
    "        \n",
    "        if i != 0 and i % 100 == 0:\n",
    "            print(f\"\\t train batch: {i:5d}/{num_batches} \\t loss: {loss.item():.3f} \\t batch_time_for_100 : {batch_mins}m {batch_secs}s\")\n",
    "            total_batch_time = 0\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86d9a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_function):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    num_batches = len(iterator)\n",
    "    total_batch_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            batch_start_time = time()\n",
    "\n",
    "            src = batch['input_ids'].to(device)\n",
    "            trg = batch['labels'].to(device)\n",
    "            \n",
    "            output = model(src, trg[:, :-1]) \n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = loss_function(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            batch_end_time = time()\n",
    "            batch_time = batch_end_time - batch_start_time\n",
    "            total_batch_time += batch_time\n",
    "\n",
    "        batch_mins = int(total_batch_time / 60)\n",
    "        batch_secs = int(total_batch_time% 60)\n",
    "\n",
    "        print(f\"\\t eval batch: {i:5d}/{num_batches} \\t loss: {loss.item():.3f} \\t batch_time_for_100 : {batch_mins}m {batch_secs}s\")\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91c0a9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch : 1]\n",
      "\t train batch:   100/3199 \t loss: 10.101 \t batch_time_for_100 : 0m 15s\n",
      "\t train batch:   200/3199 \t loss: 8.004 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 6.458 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 6.172 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 6.109 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 6.082 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 5.907 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 5.739 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 5.641 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 5.467 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 5.228 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 5.144 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 4.919 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 4.861 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 4.682 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 4.463 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 4.451 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 4.192 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 4.121 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 4.070 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 3.788 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 3.817 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 3.622 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 3.507 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 3.476 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 3.355 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 3.411 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 3.282 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 3.100 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 3.114 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 2.980 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 2.868 \t batch_time_for_100 : 0m 15s\n",
      "epoch:   1/30 \t train ppl: 131.847 val ppl: 17.899 \t 8m 19s \n",
      "\n",
      "[epoch : 2]\n",
      "\t train batch:   100/3199 \t loss: 3.050 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 2.900 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 2.914 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 2.777 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 2.838 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 2.735 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 2.788 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 2.727 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 2.646 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 2.726 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 2.551 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 2.637 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 2.555 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 2.574 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 2.596 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 2.557 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 2.455 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 2.465 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 2.329 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 2.374 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 2.397 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 2.318 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 2.379 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 2.259 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 2.286 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 2.402 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 2.224 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 2.125 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 2.240 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 2.113 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 2.284 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 2.093 \t batch_time_for_100 : 0m 15s\n",
      "epoch:   2/30 \t train ppl: 12.506 val ppl: 7.643 \t 8m 13s \n",
      "\n",
      "[epoch : 3]\n",
      "\t train batch:   100/3199 \t loss: 2.124 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 2.146 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 2.098 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 2.136 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 2.049 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 2.049 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 2.108 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 2.067 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 2.152 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 2.099 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 2.099 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 2.124 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 2.093 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.935 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 2.018 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 2.057 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.971 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 2.071 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 2.013 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.951 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.940 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.914 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.912 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.959 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.860 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.859 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.816 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.884 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.868 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.871 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.799 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.770 \t batch_time_for_100 : 0m 15s\n",
      "epoch:   3/30 \t train ppl: 7.352 val ppl: 5.486 \t 8m 15s \n",
      "\n",
      "[epoch : 4]\n",
      "\t train batch:   100/3199 \t loss: 1.681 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.831 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.848 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.795 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.792 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.803 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.829 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.819 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.813 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.685 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.730 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.665 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.656 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.729 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.702 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.745 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.701 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.778 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.734 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.806 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.672 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.647 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.683 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.674 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.693 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.750 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.634 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.678 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.655 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.614 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.706 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.571 \t batch_time_for_100 : 0m 15s\n",
      "epoch:   4/30 \t train ppl: 5.702 val ppl: 4.557 \t 8m 12s \n",
      "\n",
      "[epoch : 5]\n",
      "\t train batch:   100/3199 \t loss: 1.521 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.675 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.569 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.599 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.556 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.511 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.583 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.601 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.715 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.640 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.588 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.666 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.459 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.524 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.630 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.618 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.588 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.605 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.609 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.558 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.672 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.582 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.540 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.613 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.515 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.489 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.524 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.563 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.492 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.540 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.584 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.456 \t batch_time_for_100 : 0m 15s\n",
      "epoch:   5/30 \t train ppl: 4.885 val ppl: 4.068 \t 8m 18s \n",
      "\n",
      "[epoch : 6]\n",
      "\t train batch:   100/3199 \t loss: 1.473 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.532 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.466 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.422 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.455 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.545 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.469 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.497 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.530 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.509 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.430 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.577 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.412 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.488 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.499 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.507 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.426 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.371 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.498 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.478 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.405 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.411 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.502 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.499 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.438 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.442 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.436 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.555 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.435 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.461 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.476 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.383 \t batch_time_for_100 : 0m 15s\n",
      "epoch:   6/30 \t train ppl: 4.398 val ppl: 3.780 \t 8m 18s \n",
      "\n",
      "[epoch : 7]\n",
      "\t train batch:   100/3199 \t loss: 1.384 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.389 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.483 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.416 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.419 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.399 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.356 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.369 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.424 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.434 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.363 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.456 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.448 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.426 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.360 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.408 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.518 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.440 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.471 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.452 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.333 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.368 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.358 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.343 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.426 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.417 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.449 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.289 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.424 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.323 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.340 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.336 \t batch_time_for_100 : 0m 15s\n",
      "epoch:   7/30 \t train ppl: 4.079 val ppl: 3.590 \t 8m 17s \n",
      "\n",
      "[epoch : 8]\n",
      "\t train batch:   100/3199 \t loss: 1.374 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.299 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.343 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.357 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.367 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.334 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.485 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.391 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.330 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.262 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.368 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.411 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.381 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.388 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.328 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.346 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.379 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.280 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.381 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.315 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.387 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.336 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.307 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.413 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.436 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.277 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.310 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.416 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.344 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.323 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.356 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.285 \t batch_time_for_100 : 0m 15s\n",
      "epoch:   8/30 \t train ppl: 3.852 val ppl: 3.433 \t 8m 16s \n",
      "\n",
      "[epoch : 9]\n",
      "\t train batch:   100/3199 \t loss: 1.398 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.234 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.372 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.360 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.315 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.312 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.352 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.307 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.318 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.331 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.289 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.330 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.246 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.343 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.328 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.312 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.287 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.243 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.333 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.418 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.272 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.274 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.308 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.259 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.309 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.365 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.308 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.374 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.307 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.267 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.436 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.254 \t batch_time_for_100 : 0m 15s\n",
      "epoch:   9/30 \t train ppl: 3.679 val ppl: 3.332 \t 8m 7s \n",
      "\n",
      "[epoch : 10]\n",
      "\t train batch:   100/3199 \t loss: 1.207 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.252 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.354 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.249 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.297 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.252 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.266 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.270 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.242 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.255 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.286 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.267 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.242 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.232 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.279 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.263 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.205 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.257 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.213 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.326 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.293 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.258 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.227 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.315 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.288 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.300 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.300 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.326 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.274 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.235 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.294 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.223 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  10/30 \t train ppl: 3.543 val ppl: 3.247 \t 8m 8s \n",
      "\n",
      "[epoch : 11]\n",
      "\t train batch:   100/3199 \t loss: 1.252 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.192 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.287 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.230 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.267 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.222 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.200 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.189 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.250 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.236 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.275 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.221 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.173 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.213 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.292 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.174 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.260 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.215 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.284 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.219 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.118 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.194 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.230 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.335 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.326 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.175 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.260 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.200 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.282 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.232 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.211 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.198 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  11/30 \t train ppl: 3.433 val ppl: 3.176 \t 8m 16s \n",
      "\n",
      "[epoch : 12]\n",
      "\t train batch:   100/3199 \t loss: 1.153 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.214 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.194 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.222 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.264 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.183 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.177 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.288 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.261 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.171 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.265 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.128 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.147 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.144 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.191 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.193 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.155 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.163 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.220 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.172 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.219 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.187 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.231 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.228 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.171 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.260 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.160 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.289 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.175 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.235 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.217 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.188 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  12/30 \t train ppl: 3.340 val ppl: 3.122 \t 8m 9s \n",
      "\n",
      "[epoch : 13]\n",
      "\t train batch:   100/3199 \t loss: 1.243 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.193 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.173 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.178 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.239 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.158 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.188 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.118 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.206 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.211 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.210 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.176 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.179 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.167 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.099 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.117 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.292 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.138 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.121 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.214 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.190 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.201 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.222 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.132 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.186 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.144 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.187 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.143 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.182 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.179 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.234 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.183 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  13/30 \t train ppl: 3.262 val ppl: 3.074 \t 8m 9s \n",
      "\n",
      "[epoch : 14]\n",
      "\t train batch:   100/3199 \t loss: 1.150 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.233 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.189 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.166 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.161 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.231 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.190 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.152 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.114 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.220 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.183 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.225 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.095 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.185 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.167 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.210 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.245 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.172 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.148 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.165 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.170 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.089 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.208 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.158 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.139 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.185 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.130 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.047 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.090 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.176 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.137 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.162 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  14/30 \t train ppl: 3.194 val ppl: 3.029 \t 8m 9s \n",
      "\n",
      "[epoch : 15]\n",
      "\t train batch:   100/3199 \t loss: 1.124 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.069 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.100 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.144 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.147 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.158 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.126 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.200 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.125 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.123 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.159 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.198 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.136 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.120 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.144 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.054 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.161 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.113 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.201 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.115 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.148 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.093 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.145 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.158 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.119 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.160 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.205 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.102 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.119 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.121 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.127 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.148 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  15/30 \t train ppl: 3.134 val ppl: 2.996 \t 8m 9s \n",
      "\n",
      "[epoch : 16]\n",
      "\t train batch:   100/3199 \t loss: 1.037 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.196 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.026 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.091 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.234 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.177 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.068 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.063 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.124 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.133 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.172 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.115 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.105 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.087 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.203 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.105 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.108 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.075 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.108 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.119 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.164 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.161 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.075 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.122 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.084 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.085 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.077 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.148 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.167 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.065 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.114 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.138 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  16/30 \t train ppl: 3.081 val ppl: 2.965 \t 8m 9s \n",
      "\n",
      "[epoch : 17]\n",
      "\t train batch:   100/3199 \t loss: 1.083 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.092 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.077 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.148 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.019 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.119 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.094 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.138 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.007 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.124 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.108 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.144 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.073 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.153 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.160 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.024 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.071 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.090 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.141 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.157 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.130 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.218 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.161 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.168 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.081 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.063 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.125 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.072 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.086 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.126 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.123 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.138 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  17/30 \t train ppl: 3.034 val ppl: 2.944 \t 8m 9s \n",
      "\n",
      "[epoch : 18]\n",
      "\t train batch:   100/3199 \t loss: 0.979 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.132 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.098 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.087 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.094 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.083 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.036 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 0.990 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.113 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.002 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.139 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.084 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.128 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.135 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.107 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.127 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.101 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.057 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.071 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.100 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.050 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.123 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.143 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.114 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.109 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.098 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.084 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.094 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.055 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.027 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.055 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.116 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  18/30 \t train ppl: 2.992 val ppl: 2.919 \t 8m 9s \n",
      "\n",
      "[epoch : 19]\n",
      "\t train batch:   100/3199 \t loss: 1.011 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.053 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.124 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.056 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.016 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.160 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.047 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.098 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 0.993 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.135 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.102 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.108 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.099 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.110 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.048 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.101 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.083 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.099 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.057 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.156 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.092 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.125 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.101 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.045 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.076 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.067 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.148 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.144 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.023 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 0.997 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.151 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.117 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  19/30 \t train ppl: 2.955 val ppl: 2.905 \t 8m 9s \n",
      "\n",
      "[epoch : 20]\n",
      "\t train batch:   100/3199 \t loss: 1.034 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.126 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.112 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.002 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.038 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.055 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.102 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.045 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.071 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.048 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.125 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.055 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.156 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.010 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.069 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.076 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.093 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.132 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.062 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.022 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.054 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.105 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.025 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 0.983 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.035 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.003 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.034 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.046 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.009 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.114 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.090 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.110 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  20/30 \t train ppl: 2.918 val ppl: 2.880 \t 8m 10s \n",
      "\n",
      "[epoch : 21]\n",
      "\t train batch:   100/3199 \t loss: 1.060 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.070 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.113 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.075 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.026 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.028 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.090 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 0.986 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.065 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.154 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.023 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.002 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.052 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.089 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.091 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.130 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.122 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.008 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.114 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.052 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.037 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.050 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.024 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.019 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.139 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.047 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.034 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.041 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.072 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.072 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.078 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.107 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  21/30 \t train ppl: 2.887 val ppl: 2.862 \t 8m 9s \n",
      "\n",
      "[epoch : 22]\n",
      "\t train batch:   100/3199 \t loss: 1.014 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.018 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.082 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 0.944 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.069 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.057 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.043 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 0.981 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.000 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.126 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.124 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.054 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.122 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.071 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.026 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.072 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.189 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.098 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.003 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.132 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.051 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.048 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 0.945 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.065 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.045 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.111 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.048 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.092 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 0.981 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.147 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.070 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.102 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  22/30 \t train ppl: 2.857 val ppl: 2.847 \t 8m 9s \n",
      "\n",
      "[epoch : 23]\n",
      "\t train batch:   100/3199 \t loss: 1.085 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.067 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.062 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.091 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.082 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.034 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.061 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.082 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.005 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.010 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.084 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.002 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 0.984 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.004 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 0.997 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.017 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.057 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.079 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.182 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.002 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.059 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.014 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.061 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.050 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.035 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 0.989 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.086 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.093 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.036 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.027 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.089 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.107 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  23/30 \t train ppl: 2.830 val ppl: 2.841 \t 8m 9s \n",
      "\n",
      "[epoch : 24]\n",
      "\t train batch:   100/3199 \t loss: 0.936 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.033 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.067 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.022 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.078 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.020 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 0.982 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.085 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.133 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.015 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.024 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.067 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.061 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.133 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.049 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.014 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.069 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.089 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.032 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.017 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.069 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 0.967 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.075 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.085 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.071 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 0.990 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.000 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.056 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.034 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.020 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.010 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.101 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  24/30 \t train ppl: 2.805 val ppl: 2.821 \t 8m 9s \n",
      "\n",
      "[epoch : 25]\n",
      "\t train batch:   100/3199 \t loss: 1.072 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.011 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 0.952 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 1.056 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 0.950 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.101 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.032 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 0.969 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 0.997 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.051 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 0.995 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.061 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.003 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.044 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.016 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.036 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.026 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.028 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.060 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.036 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 0.976 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.009 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.039 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.020 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.166 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.062 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 0.971 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.063 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.026 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 0.962 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 0.992 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.091 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  25/30 \t train ppl: 2.780 val ppl: 2.807 \t 8m 12s \n",
      "\n",
      "[epoch : 26]\n",
      "\t train batch:   100/3199 \t loss: 0.954 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.040 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.034 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 0.970 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 0.988 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.012 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.043 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 0.994 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.001 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.051 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 0.958 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.026 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 0.985 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 0.995 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 0.976 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.034 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 0.977 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.039 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.072 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 0.982 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.048 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.042 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 0.971 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 0.952 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 0.976 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 0.973 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 0.940 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 0.995 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 0.973 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 0.969 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.045 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.088 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  26/30 \t train ppl: 2.758 val ppl: 2.795 \t 8m 18s \n",
      "\n",
      "[epoch : 27]\n",
      "\t train batch:   100/3199 \t loss: 1.043 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 0.989 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.044 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 0.993 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.060 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 1.026 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 0.986 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 1.020 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 0.911 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.002 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 0.989 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.007 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.020 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.043 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.088 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.069 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 0.984 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.001 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.040 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.062 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.076 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.072 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 0.951 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.030 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 1.065 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 0.957 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 0.997 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 0.995 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.064 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.075 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 0.976 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.086 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  27/30 \t train ppl: 2.738 val ppl: 2.790 \t 8m 18s \n",
      "\n",
      "[epoch : 28]\n",
      "\t train batch:   100/3199 \t loss: 0.936 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 0.846 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 1.056 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 0.999 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 0.892 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 0.987 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.025 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 0.989 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 0.993 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 0.973 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 0.974 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 0.999 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.077 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.005 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 1.008 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.021 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.014 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 1.043 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 0.938 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 0.992 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.048 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.039 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 0.990 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 1.025 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 0.985 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.095 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 0.974 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 0.957 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 0.985 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.014 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.037 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.079 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  28/30 \t train ppl: 2.718 val ppl: 2.783 \t 8m 17s \n",
      "\n",
      "[epoch : 29]\n",
      "\t train batch:   100/3199 \t loss: 0.971 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 1.023 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 0.964 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 0.966 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 1.007 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 0.943 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 1.008 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 0.993 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 0.969 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 0.909 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 1.020 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.040 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 1.069 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.000 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 0.941 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 1.049 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 1.032 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 0.981 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 0.962 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 0.969 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 1.006 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 1.029 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.024 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 0.990 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 0.985 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.017 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 1.029 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 1.031 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 1.002 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 0.976 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 1.042 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.093 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  29/30 \t train ppl: 2.698 val ppl: 2.767 \t 8m 18s \n",
      "\n",
      "[epoch : 30]\n",
      "\t train batch:   100/3199 \t loss: 0.970 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   200/3199 \t loss: 0.987 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   300/3199 \t loss: 0.933 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   400/3199 \t loss: 0.980 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   500/3199 \t loss: 0.938 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   600/3199 \t loss: 0.972 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   700/3199 \t loss: 0.897 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   800/3199 \t loss: 0.940 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:   900/3199 \t loss: 1.008 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1000/3199 \t loss: 1.005 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1100/3199 \t loss: 0.961 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1200/3199 \t loss: 1.003 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1300/3199 \t loss: 0.975 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1400/3199 \t loss: 1.060 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1500/3199 \t loss: 0.925 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1600/3199 \t loss: 0.919 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1700/3199 \t loss: 0.963 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1800/3199 \t loss: 0.962 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  1900/3199 \t loss: 1.039 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2000/3199 \t loss: 1.060 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2100/3199 \t loss: 0.960 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2200/3199 \t loss: 0.983 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2300/3199 \t loss: 1.009 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2400/3199 \t loss: 0.984 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2500/3199 \t loss: 0.971 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2600/3199 \t loss: 1.020 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2700/3199 \t loss: 0.991 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2800/3199 \t loss: 0.938 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  2900/3199 \t loss: 0.984 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3000/3199 \t loss: 1.056 \t batch_time_for_100 : 0m 14s\n",
      "\t train batch:  3100/3199 \t loss: 0.991 \t batch_time_for_100 : 0m 14s\n",
      "\t eval batch:   355/356 \t loss: 1.077 \t batch_time_for_100 : 0m 15s\n",
      "epoch:  30/30 \t train ppl: 2.681 val ppl: 2.766 \t 8m 18s \n",
      "\n",
      "\n",
      "total time : 14796.283065319061\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    print(f\"[epoch : {epoch}]\")\n",
    "    start_time = time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, loss_function)\n",
    "    valid_loss = evaluate(model, valid_iterator, loss_function)\n",
    "    \n",
    "    train_ppl = math.exp(train_loss)\n",
    "    valid_ppl = math.exp(valid_loss)\n",
    "\n",
    "    end_time = time()\n",
    "    \n",
    "    epoch_time = end_time - start_time\n",
    "    total_time += epoch_time\n",
    "    \n",
    "    epoch_mins = int((epoch_time) / 60)\n",
    "    epoch_secs = int((epoch_time) % 60)\n",
    "\n",
    "    print(f\"epoch: {epoch:3d}/{epochs} \\t train ppl: {train_ppl:4.3f} val ppl: {valid_ppl:4.3f} \\t {epoch_mins}m {epoch_secs}s \\n\")\n",
    "\n",
    "print(f\"\\ntotal time : {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f406601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_translation(model, sentence, tokenizer, max_len=64):\n",
    "    model.eval()\n",
    "\n",
    "    tokenized = tokenizer(sentence, return_tensors='pt', truncation=True, max_length=max_len)\n",
    "\n",
    "    src = tokenized['input_ids'].to(device)\n",
    "\n",
    "    sos_token_id = tokenizer.pad_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    trg_tokens = torch.tensor([[sos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_mask = (src != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        enc_src = model.encoder(src, src_mask)\n",
    "\n",
    "    for i in range(max_len - 1):\n",
    "        with torch.no_grad():\n",
    "            _, trg_mask = create_masks(src, trg_tokens, pad_token_id)\n",
    "            dec_output = model.decoder(trg_tokens, enc_src, trg_mask, src_mask)\n",
    "            output = model.fully_connected_layer(dec_output)\n",
    "\n",
    "        output_token = output[:, -1, :] \n",
    "\n",
    "        next_token_id = output_token.argmax(1).item()\n",
    "\n",
    "        trg_tokens = torch.cat(\n",
    "            [trg_tokens, torch.tensor([[next_token_id]], dtype=torch.long, device=device)], dim = 1)\n",
    "\n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "\n",
    "    translated_tokens = trg_tokens.squeeze(0).tolist()[1:]\n",
    "\n",
    "    translated_sentence = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence: I am happy. Because studing is hard.\n",
      "output sequence: uern, weil die Zulassung schwierig ist.\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I am happy. Because studing is hard.\" \n",
    "\n",
    "predicted_translation = predict_translation(model, test_sentence, tokenizer)\n",
    "\n",
    "print(f\"input sequence: {test_sentence}\")\n",
    "print(f\"output sequence: {predicted_translation}\")\n",
    "# 제대로 학습되었다면 : Ich bin glücklich. Denn das Studium ist anstrengend. (또는 유사한 번역)\n",
    "# 번역 결과 : uern, weil die Zulassung schwierig ist. (지불하는 이유는 승인 절차가 어렵기 때문입니다....?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
