{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5231e34c",
   "metadata": {},
   "source": [
    "# VIsion Transformer (ViT)\n",
    "\n",
    "- image 를 word sequence 처럼 취급 -> input image 를 patch (word -> token 처럼) 로 분할 후 transformer에 입력\n",
    "\n",
    "- 구조 : Imaqge Patching -> Embedding -> Transformer (Encoder Only) \n",
    "\n",
    "    Image Patching : image를 작은 patch로 분할하는 작업\n",
    "\n",
    "    Embedding : patch를 Flatten -> patch embedding vector 로 변환 -> class token embedding 추가 -> positional embedding 추가\n",
    "\n",
    "    Transformer : multi-head self-attention 으로 patch들의 관계 학습 -> class token vector를 통해 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b55bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor,Normalize\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.nn import Module, Dropout, Linear, CrossEntropyLoss, init, utils, LayerNorm, ModuleList, Conv2d, Parameter\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from time import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59c93e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.xpu.is_available():\n",
    "    device = torch.device(\"xpu\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else: \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2698bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "train_data = CIFAR10('data_cifar10', train=True, transform=transform)\n",
    "valid_data = CIFAR10('data_cifar10', train=False, transform=transform)\n",
    "test_data = CIFAR10('data_cifar10', train=False, transform=transform)\n",
    "\n",
    "batch_size = 256\n",
    "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_iterator = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21d35cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Module):\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        position_encoding_matrics = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
    "        position_encoding_matrics[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding_matrics[:, 1::2] = torch.cos(position * div_term)\n",
    "        position_encoding_matrics = position_encoding_matrics.unsqueeze(0)\n",
    "        self.register_buffer('position_encoding_matrics', position_encoding_matrics)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input sequence의 길이로 잘라서 리턴 (ViT에서는 [CLS] + N_patches)\n",
    "        return self.position_encoding_matrics[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd7a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Module):\n",
    "    def __init__(self, model_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = model_dim // n_heads \n",
    "        self.fc_q = Linear(model_dim, model_dim)\n",
    "        self.fc_k = Linear(model_dim, model_dim)\n",
    "        self.fc_v = Linear(model_dim, model_dim)\n",
    "        self.fc_o = Linear(model_dim, model_dim)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        q = self.fc_q(query)\n",
    "        k = self.fc_k(key)\n",
    "        v = self.fc_v(value)\n",
    "        \n",
    "        q = q.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        energy = torch.matmul(q, k.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        x = torch.matmul(self.dropout(attention), v)\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.reshape(batch_size, -1, self.model_dim)\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7bc7ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforward(Module):\n",
    "    def __init__(self, model_dim, feedforward_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = Linear(model_dim, feedforward_dim)\n",
    "        self.fc_2 = Linear(feedforward_dim, model_dim)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c13902fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection_PreNorm(Module):\n",
    "    def __init__(self, model_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(model_dim)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2a4b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Module):\n",
    "    def __init__(self, model_dim, n_heads, feedforward_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(model_dim, n_heads, dropout)\n",
    "        self.self_attention_sublayer = SublayerConnection_PreNorm(model_dim, dropout) \n",
    "        self.feed_forward = PositionwiseFeedforward(model_dim, feedforward_dim, dropout)\n",
    "        self.feedforward_sublayer = SublayerConnection_PreNorm(model_dim, dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = self.self_attention_sublayer(src, lambda x: self.self_attention(x, x, x)[0])\n",
    "        src = self.feedforward_sublayer(src, self.feed_forward)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a23e67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "    def __init__(self, model_dim, n_layers, n_heads, feedforward_dim, dropout):\n",
    "        super().__init__()\n",
    "        # ViT 에서는 embedding과 positional encoding을 ViT에서 처리\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.layers = ModuleList([\n",
    "            EncoderLayer(model_dim, n_heads, feedforward_dim, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        # ViT 는 encoder only -> 마지막 출력이 된다.\n",
    "        self.norm = LayerNorm(model_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src) \n",
    "            \n",
    "        return self.norm(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "659a3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_classes, model_dim, n_layers, n_heads, feedforward_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # patch embedding : image -> patch -> embedding vector\n",
    "        # patch size와 stride를 똑같이 설정 = 겹치지 않는 patch 생성\n",
    "        self.patch_embedding = Conv2d(\n",
    "            in_channels, model_dim, \n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # classification을 위해 label을 token으로 연결 및 초기화\n",
    "        self.class_token = Parameter(torch.zeros(1, 1, model_dim))\n",
    "        init.trunc_normal_(self.class_token, std=.02)\n",
    "        \n",
    "        # Position Encoding (num_patches + 1 길이)\n",
    "        self.position_embedding = PositionalEncoding(model_dim, max_len=self.num_patches + 1)\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        # transformer의 encoder 사용\n",
    "        self.transformer_encoder = Encoder(model_dim, n_layers, n_heads, feedforward_dim, dropout)\n",
    "        \n",
    "        # classifier 역할을 수행하는 head\n",
    "        self.classifier_head = Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # class token 추가\n",
    "        class_tokens = self.class_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((class_tokens, x), dim=1)\n",
    "        \n",
    "        # positional encoding 추가\n",
    "        x = self.dropout(x + self.position_embedding(x))\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # classification\n",
    "        class_output = x[:, 0]\n",
    "        output = self.classifier_head(class_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99fcc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "# 4x4 패치 -> (32/4)^2 = 64개 패치 시퀀스\n",
    "patch_size = 4\n",
    "in_channels = 3\n",
    "# 0-9 클래스\n",
    "output_dim = 10\n",
    "model_dim = 512\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "feedforward_dim = model_dim * 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c6acc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(img_size, patch_size, in_channels, output_dim, model_dim, n_layers, n_heads, feedforward_dim, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "021eda30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embedding): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))\n",
       "  (position_embedding): PositionalEncoding()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (transformer_encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (self_attention_sublayer): SublayerConnection_PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedforward(\n",
       "          (fc_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (feedforward_sublayer): SublayerConnection_PreNorm(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier_head): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 초기화\n",
    "# layer에 따라 다른 가중치 초기화 기법 사용\n",
    "def init_weights(model):\n",
    "    # Linear 일 경우 : Truncated Normal Initialization\n",
    "    if isinstance(model, Linear):\n",
    "        init.trunc_normal_(model.weight, std=.02)\n",
    "        if model.bias is not None:\n",
    "            init.constant_(model.bias, 0)\n",
    "    # Layer Normalization 일 경우 : weight 는 1 / bias 는 0\n",
    "    elif isinstance(model, LayerNorm):\n",
    "        init.constant_(model.bias, 0)\n",
    "        init.constant_(model.weight, 1.0)\n",
    "\n",
    "    # Convolution 일 경우 : Kaiming Normal Initialization (카이밍 정규분포 초기화)\n",
    "    elif isinstance(model, Conv2d):\n",
    "        init.kaiming_normal_(model.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if model.bias is not None:\n",
    "            init.constant_(model.bias, 0)\n",
    "    \n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddf9de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOptimizer:\n",
    "    def __init__(self, model_dim, warmup_steps, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.model_dim = model_dim\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self._step = 0\n",
    "        self._rate = 0.\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for param in self.optimizer.param_groups:\n",
    "            param['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        \n",
    "        scale = self.model_dim ** (-0.5)\n",
    "        \n",
    "        return scale * min(step ** (-0.5), step * (self.warmup_steps ** (-1.5)))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58c832f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epsilon = 1e-9\n",
    "betas=(0.9, 0.98)\n",
    "weight_decay = 0.01 \n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, betas=betas, eps=epsilon, weight_decay=weight_decay)\n",
    "\n",
    "warmup_steps = 2000\n",
    "optimizer = NoamOptimizer(model_dim, warmup_steps, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52dad380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_function):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for images, labels in iterator:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = loss_function(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b56b0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_function):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in iterator:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(images) \n",
    "            loss = loss_function(output, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds = output.argmax(dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    return epoch_loss / len(iterator), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ca45813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1/100 \t train loss: 2.243 val loss: 2.112 \t val acc: 20.38% \t 0m 28s\n",
      "epoch:   2/100 \t train loss: 1.886 val loss: 1.651 \t val acc: 40.48% \t 0m 28s\n",
      "epoch:   3/100 \t train loss: 1.646 val loss: 1.538 \t val acc: 44.62% \t 0m 28s\n",
      "epoch:   4/100 \t train loss: 1.517 val loss: 1.489 \t val acc: 46.96% \t 0m 28s\n",
      "epoch:   5/100 \t train loss: 1.440 val loss: 1.456 \t val acc: 47.40% \t 0m 28s\n",
      "epoch:   6/100 \t train loss: 1.389 val loss: 1.377 \t val acc: 50.44% \t 0m 28s\n",
      "epoch:   7/100 \t train loss: 1.346 val loss: 1.347 \t val acc: 52.40% \t 0m 28s\n",
      "epoch:   8/100 \t train loss: 1.311 val loss: 1.318 \t val acc: 53.07% \t 0m 28s\n",
      "epoch:   9/100 \t train loss: 1.302 val loss: 1.329 \t val acc: 52.87% \t 0m 28s\n",
      "epoch:  10/100 \t train loss: 1.296 val loss: 1.275 \t val acc: 54.67% \t 0m 28s\n",
      "epoch:  11/100 \t train loss: 1.283 val loss: 1.239 \t val acc: 55.49% \t 0m 28s\n",
      "epoch:  12/100 \t train loss: 1.243 val loss: 1.244 \t val acc: 55.84% \t 0m 28s\n",
      "epoch:  13/100 \t train loss: 1.196 val loss: 1.142 \t val acc: 58.64% \t 0m 28s\n",
      "epoch:  14/100 \t train loss: 1.155 val loss: 1.140 \t val acc: 58.33% \t 0m 28s\n",
      "epoch:  15/100 \t train loss: 1.117 val loss: 1.132 \t val acc: 59.50% \t 0m 28s\n",
      "epoch:  16/100 \t train loss: 1.079 val loss: 1.074 \t val acc: 61.91% \t 0m 28s\n",
      "epoch:  17/100 \t train loss: 1.044 val loss: 1.070 \t val acc: 62.04% \t 0m 28s\n",
      "epoch:  18/100 \t train loss: 1.015 val loss: 1.030 \t val acc: 63.20% \t 0m 28s\n",
      "epoch:  19/100 \t train loss: 0.976 val loss: 1.054 \t val acc: 63.04% \t 0m 28s\n",
      "epoch:  20/100 \t train loss: 0.946 val loss: 1.009 \t val acc: 64.76% \t 0m 28s\n",
      "epoch:  21/100 \t train loss: 0.917 val loss: 0.995 \t val acc: 65.37% \t 0m 28s\n",
      "epoch:  22/100 \t train loss: 0.886 val loss: 0.999 \t val acc: 65.82% \t 0m 28s\n",
      "epoch:  23/100 \t train loss: 0.858 val loss: 0.988 \t val acc: 65.64% \t 0m 28s\n",
      "epoch:  24/100 \t train loss: 0.830 val loss: 0.958 \t val acc: 67.03% \t 0m 28s\n",
      "epoch:  25/100 \t train loss: 0.802 val loss: 0.914 \t val acc: 68.22% \t 0m 28s\n",
      "epoch:  26/100 \t train loss: 0.776 val loss: 0.928 \t val acc: 68.15% \t 0m 28s\n",
      "epoch:  27/100 \t train loss: 0.754 val loss: 0.929 \t val acc: 68.00% \t 0m 28s\n",
      "epoch:  28/100 \t train loss: 0.726 val loss: 0.941 \t val acc: 68.40% \t 0m 28s\n",
      "epoch:  29/100 \t train loss: 0.701 val loss: 0.907 \t val acc: 69.44% \t 0m 28s\n",
      "epoch:  30/100 \t train loss: 0.671 val loss: 0.948 \t val acc: 68.90% \t 0m 28s\n",
      "epoch:  31/100 \t train loss: 0.644 val loss: 0.916 \t val acc: 69.78% \t 0m 28s\n",
      "epoch:  32/100 \t train loss: 0.623 val loss: 0.945 \t val acc: 68.90% \t 0m 28s\n",
      "epoch:  33/100 \t train loss: 0.598 val loss: 0.973 \t val acc: 68.98% \t 0m 28s\n",
      "epoch:  34/100 \t train loss: 0.576 val loss: 0.923 \t val acc: 70.04% \t 0m 28s\n",
      "epoch:  35/100 \t train loss: 0.554 val loss: 0.984 \t val acc: 69.52% \t 0m 28s\n",
      "epoch:  36/100 \t train loss: 0.531 val loss: 0.951 \t val acc: 70.57% \t 0m 28s\n",
      "epoch:  37/100 \t train loss: 0.509 val loss: 0.966 \t val acc: 70.35% \t 0m 28s\n",
      "epoch:  38/100 \t train loss: 0.485 val loss: 0.997 \t val acc: 70.58% \t 0m 28s\n",
      "epoch:  39/100 \t train loss: 0.466 val loss: 0.987 \t val acc: 70.63% \t 0m 28s\n",
      "epoch:  40/100 \t train loss: 0.439 val loss: 1.031 \t val acc: 70.58% \t 0m 28s\n",
      "epoch:  41/100 \t train loss: 0.422 val loss: 1.031 \t val acc: 70.81% \t 0m 28s\n",
      "epoch:  42/100 \t train loss: 0.405 val loss: 1.041 \t val acc: 70.62% \t 0m 28s\n",
      "epoch:  43/100 \t train loss: 0.387 val loss: 1.033 \t val acc: 71.39% \t 0m 28s\n",
      "EarlyStopping (patience : 15)\n",
      "total time : 1225.1598773002625 \t total acc : 61.72%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "total_time = 0\n",
    "total_acc = list()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "counter = 0\n",
    "min_delta = 0.001\n",
    "best_model_path = 'vit_cifar10_best.pt'\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, loss_function)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_function)\n",
    "\n",
    "    # early stopping\n",
    "    if valid_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = valid_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "    if counter >= patience:\n",
    "        print(f\"EarlyStopping (patience : {patience})\")\n",
    "        break\n",
    "\n",
    "    end_time = time()\n",
    "    \n",
    "    epoch_time = end_time - start_time\n",
    "    total_time += epoch_time\n",
    "\n",
    "    epoch_mins = int((epoch_time) / 60)\n",
    "    epoch_secs = int((epoch_time) % 60)\n",
    "\n",
    "    total_acc.append(valid_acc)\n",
    "\n",
    "    print(f\"epoch: {epoch:3d}/{epochs} \\t train loss: {train_loss:4.3f} val loss: {valid_loss:4.3f} \\t val acc: {valid_acc*100:5.2f}% \\t {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "\n",
    "if total_acc:\n",
    "    avg_total_acc = sum(total_acc) / len(total_acc)\n",
    "    # early stopping으로 인해 patience 이후의 가중치(=overfitting) 대신 최적의 모델 가중치 사용\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "else:\n",
    "    avg_total_acc = 0.0\n",
    "\n",
    "print(f\"total time : {total_time} \\t total acc : {avg_total_acc*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96e5b2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation - loss: 0.907 \t acc: 69.44%\n"
     ]
    }
   ],
   "source": [
    "test_iterator = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_function)\n",
    "\n",
    "print(f\"evaluation - loss: {test_loss:4.3f} \\t acc: {test_acc*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4ba094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image, transform):\n",
    "    model.eval()\n",
    "    \n",
    "    if not torch.is_tensor(image):\n",
    "        image = transform(image)\n",
    "    \n",
    "    # batch dim 추가 (c, h, w) -> (1, c, h, w)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predict = model(image)\n",
    "    \n",
    "    predicted_class = predict.argmax(dim=1).item()\n",
    "    \n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e3e53f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label : cat      \t predict : cat      \t True\n",
      "label : ship     \t predict : ship     \t True\n",
      "label : ship     \t predict : ship     \t True\n",
      "label : plane    \t predict : plane    \t True\n",
      "label : frog     \t predict : frog     \t True\n",
      "label : frog     \t predict : frog     \t True\n",
      "label : car      \t predict : car      \t True\n",
      "label : frog     \t predict : frog     \t True\n",
      "label : cat      \t predict : deer     \t False\n",
      "label : car      \t predict : car      \t True\n"
     ]
    }
   ],
   "source": [
    "class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "for i in range(10):\n",
    "    test_image_pil, label_index = test_data[i]\n",
    "\n",
    "    predict = predict_image(model, test_image_pil, transform)\n",
    "\n",
    "    label_name = class_names[label_index]\n",
    "    predict_name = class_names[predict]\n",
    "\n",
    "    print(f\"label : {label_name:8} \\t predict : {predict_name:8} \\t {label_name == predict_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168e0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
